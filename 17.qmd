---
title: "Bank Term Deposit Subscription Prediction using Machine Learning"
author: "Team 17"
editor: visual
number-sections: true
format: 
  pdf:
    geometry: "left=2cm, right=2cm, top=2.5cm, bottom=2.5cm"
execute:
  eval: true
  warning: false
  message: false
---

# Introduction {#sec-intro}

Portuguese bank institution launched marketing campaign to promote term deposit subscription. These campaigns are based on phone calls, and more than one contact to the same client is performed.

Furthermore, the research question is to make prediction whether the client will subscribe (yes/no) a term deposit or not. There are 19 attributes available in the dataset including bank client data, last contact of the current campaign information, social-economic, and others.

```{r}
#| echo: false
#| warning: false
#| label: tbl-dataset-head
#| tbl-cap: First five entries of the Bank Marketing Dataset.

library(tidyverse)
library(gt)

# Read CSV from data dir
data <- read_csv("group_17.csv")

# Display the first 5 rows
data[,1:9] |> 
  slice_head(n=5) |>
  gt() |>
  cols_label(
    age = html("Age"),
    job = html("Job"),
    marital = html("Marital"),
    education = html("Education"),
    default = html("Default"),
    housing = html("Housing"),
    loan = html("Loan"),
    contact = html("Contact"),
    month = html("Month"),
  )

data[,10:16] |> 
  slice_head(n=5) |>
  gt() |>
  cols_label(
    day_of_week = html("Day of Week"),
    duration = html("Duration"),
    campaign = html("Campaign"),
    pdays = html("Pdays"),
    previous = html("Previous"),
    poutcome = html("Poutcome"),
    emp.var.rate = html("Employment Variation Rate"),
  )

data[,17:21] |> 
  slice_head(n=5) |>
  gt() |>
  cols_label(
    cons.price.idx = html("Consumer Price Index"),
    cons.conf.idx = html("Consumer Confidence Index"),
    euribor3m = html("Euribor"),
    nr.employed = html("Number of Employees"),
    y = html("Subscribe")
  )
```

```{r}
#| echo: false
#| warning: false

# Import Libraries

library(skimr)
library(knitr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(GGally)
library(caret)
library(Boruta)
```

# Exploratory Data Analysis

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-stats
#| tbl-cap: Summary statistics of the Bank Marketing Dataset.

# Creating summary statistics

# Summary statistics with adjusted skim()
my_skim <- skim_with(
  base = sfl(n = length),
  character = sfl(min=NULL,max=NULL,empty=NULL,whitespace=NULL),
  numeric = sfl(p0=NULL, p100=NULL, hist=NULL))
knit_print(my_skim(data))
```

```{r}
#| echo: false
#| warning: false

# Compute percentage of missing values

missing_percentages <- data %>%
  summarise_all(~sum(is.na(.)))/nrow(data)

missing_percentages_sorted <- missing_percentages %>%
  gather(key = "Column", value = "Missing_Percentages") %>%
  arrange(desc(Missing_Percentages)) %>%
  filter(Missing_Percentages>0)

ggplot(missing_percentages_sorted, aes(x = reorder(Column, Missing_Percentages), y = Missing_Percentages)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(title = "Percentage of Missing Values in Each Variable", x = "Variables", y = "Missing Percentages") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  coord_flip()
```

```{r}
#| echo: false
#| warning: false

# Split the dataset into Training, Validation, and Test

set.seed(123)

# Split training 80% and test 20%
index <- sample(1:nrow(data), nrow(data)*0.8)
training <- data[index, ]
test <- data[-index, ]

# Split train 60% and validation 20%
index <- sample(1:nrow(training), nrow(training)*0.75)
train <- training[index, ]
valid <- training[-index, ]

# Check the size
print(paste("Train nrows:", nrow(train)))
print(paste("Valid nrows:", nrow(valid)))
print(paste("Test nrows:", nrow(test)))
```

```{r}
#| echo: false
#| warning: false

ggplot(train, aes(x = y)) +
geom_bar() +  # Create a bar plot
labs(x = "Subscribe", y = "Count")
```

```{r}
#| echo: false
#| warning: false
#| label: fig-boxplot1
#| fig-cap: The relationship between binary response and continuous explanatory variables.
#| fig-align: center
#| fig-width: 7
#| fig-height: 10
#| message: false

# Creating Boxplot

p1 <- ggplot(training, aes(x=y, y=age)) + geom_boxplot() +
  labs(x="subscribe",y="Age")
p2 <- ggplot(training, aes(x=y, y=duration)) + geom_boxplot() +
  labs(x="subscribe",y="Duration")
p3 <- ggplot(training, aes(x=y, y=campaign)) + geom_boxplot() +
  labs(x="subscribe",y="Campaign")
p4 <- ggplot(training, aes(x=y, y=pdays)) + geom_boxplot() +
  labs(x="subscribe",y="Pdays")
p5 <- ggplot(training, aes(x=y, y=previous)) + geom_boxplot() +
  labs(x="subscribe",y="Previous")
p6 <- ggplot(training, aes(x=y, y=emp.var.rate)) + geom_boxplot() +
  labs(x="subscribe",y="Emp. Variation Rate")
p7 <- ggplot(training, aes(x=y, y=cons.price.idx)) + geom_boxplot() +
  labs(x="subscribe",y="Cons. Price Index")
p8 <- ggplot(training, aes(x=y, y=cons.conf.idx)) + geom_boxplot() +
  labs(x="subscribe",y="Cons. Confidence Index")
p9 <- ggplot(training, aes(x=y, y=euribor3m)) + geom_boxplot() +
  labs(x="subscribe",y="Euribor")
p10 <- ggplot(training, aes(x=y, y=nr.employed)) + geom_boxplot() +
  labs(x="subscribe",y="Number of Employees")

# Plot in Grid
grid.arrange(p1, p2, p3, p4, 
             p5, p6, p7, p8,
             p9, p10, ncol=2)
```

```{r}
#| echo: false
#| warning: false
#| label: fig-barplot
#| fig-cap: The relationship between binary response and categorical explanatory variables.
#| fig-align: center
#| fig-width: 8
#| fig-height: 20
#| message: false

p1 <- ggplot(training, aes(x=job, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Job", y="Percent", fill="subscribe") +
coord_flip()

p2 <- ggplot(training, aes(x=marital, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Marital", y="Percent", fill="subscribe") +
coord_flip()

p3 <- ggplot(training, aes(x=education, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Education", y="Percent", fill="subscribe") +
coord_flip()

p4 <- ggplot(training, aes(x=default, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Default", y="Percent", fill="subscribe") +
coord_flip()

p5 <- ggplot(training, aes(x=housing, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Housing", y="Percent", fill="subscribe") +
coord_flip()

p6 <- ggplot(training, aes(x=loan, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Loan", y="Percent", fill="subscribe") +
coord_flip()

p7 <- ggplot(training, aes(x=contact, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Contact", y="Percent", fill="subscribe") +
coord_flip()

p8 <- ggplot(training, aes(x=month, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Month", y="Percent", fill="subscribe") +
coord_flip()

p9 <- ggplot(training, aes(x=day_of_week, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Day of Week", y="Percent", fill="subscribe") +
coord_flip()

p10 <- ggplot(training, aes(x=poutcome, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Poutcome", y="Percent", fill="subscribe") +
coord_flip()

# Plot in Grid
grid.arrange(p1, p2, p3, p4, 
             p5, p6, p7, p8,
             p9, p10, ncol=1)
```

```{r}
#| echo: false
#| warning: false
#| label: fig-scatterplot
#| fig-cap: The relationship between continuous explanatory variables.
#| fig-align: center
#| fig-width: 12
#| fig-height: 12
#| message: false

numeric_cols <- names(select_if(training, is.numeric))

ggpairs(training,
        columns=numeric_cols,
        ggplot2::aes(colour = y),
        axisLabels = "none",
        )
```

According to the plots:

-   \[Density Plots\] There is no noticeable pattern from the continuous explanatory variables which are able to distinguish "yes" or "no" subscribe really well.

-   \[Scatter Plots\] There are some significant correlations, which are considerably moderate to strong. However, points in their scatterplots are more sparse, because the values between observations are similar to each other. Those variables belong to social-economic attributes. Should we implement PCA?

    -   nr.employed vs. emp.var.rate (0.909)

    -   euribor3m vs. emp.var.rate (0.973)

    -   cons.price.idx vs. emp.var.rate (0.779)

    -   euribor3m vs. nr.employed (0.946)

-   \[Density and Box Plots\] Some variables are skewed, is it required to standardize them?

```{r}
#| echo: false
#| warning: false

# Try using PCA

# Check standard deviation
round(sqrt(diag(cov(training[,16:20]))),1)

# Perform PCA
socio.econ.pca <- princomp(training[,16:20],cor=T)

# Display summary
summary(socio.econ.pca)
plot(socio.econ.pca)

socio.econ.pca2 <- as.data.frame(socio.econ.pca$scores[,1:2])
socio.econ.pca2$y <- training$y
ggplot(socio.econ.pca2, aes(x=c(1:nrow(training)),y=Comp.1,color=as.factor(y))) + 
  geom_jitter(width = 0.75, height = 0.75) +
  labs(x='Index', y='Comp.1', title='PCA')
```

# Data Preprocessing {#section-preprocessing}

Steps:

-   Remove `default` because \~20% of them are NAs, and the rest is only "no" for having credit in default.

-   Remove `pdays` because most of the values marked as 999.

-   Remove `duration` as this feature is available after the call, so it should not be used for prediction.

-   Remove rows containing NAs.

-   Perform PCA to socio-economic features.

-   Perform scaling to numerical features,

-   Convert categorical features as dummies.

```{r}
#| echo: false
#| warning: false

# Remove default, pdays, and duration
train <- train %>% select(-default, -pdays, -duration)
valid <- valid %>% select(-default, -pdays, -duration)
test <- test %>% select(-default, -pdays, -duration)

# Remove NAs
train <- na.omit(train)
valid <- na.omit(valid)
test <- na.omit(test)

set.seed(123)
# PCA to socio-economic features
socio.econ.pca <- princomp(train[,13:17],cor=T)
train$pca.socio.economic <- as.data.frame(socio.econ.pca$scores)$'Comp.1'
valid$pca.socio.economic <- as.data.frame(predict(socio.econ.pca,valid[,13:17]))$'Comp.1'
test$pca.socio.economic <- as.data.frame(predict(socio.econ.pca,test[,13:17]))$'Comp.1'

train <- train[, -c(13:17)]
valid <- valid[, -c(13:17)]
test <- test[, -c(13:17)]

# Scaling numerical variables
numeric_cols <- names(select_if(train, is.numeric))

var.mean <- apply(train[, numeric_cols],2,mean) #calculate mean
var.sd <- apply(train[, numeric_cols],2,sd)   #calculate standard deviation

# standardise training, validation and test sets
train.scale <- t(apply(train[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))
valid.scale <- t(apply(valid[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))
test.scale <- t(apply(test[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))

train[, numeric_cols] <- train.scale
valid[, numeric_cols] <- valid.scale
test[, numeric_cols] <- test.scale
  
# Convert categorical to dummies

# Convert response variable to be 0 and 1
train$y <- ifelse(train$y == "yes", 1, 0)
valid$y <- ifelse(valid$y == "yes", 1, 0)
test$y <- ifelse(test$y == "yes", 1, 0)

# Define one-hot encoding function
dummy <- dummyVars(" ~ .", data=train)

# Convert to dummies
train <- data.frame(predict(dummy, newdata=train))
valid <- data.frame(predict(dummy, newdata=valid))
test <- data.frame(predict(dummy, newdata=test))
```

# Model Fitting

## Decision Tree

Decision tree is a non-parametric classification method by creating partitions of the feature space (explanatory variables). After the feature space is divided into disjoint and non-overlapping regions, then the class label is predicted as the class that occurs most frequently in the region.

In this analysis, `rpart` package is being used to build the decision tree. Then, there are four hyper-parameters will be explored, such as: `maxdepth`, `minsplit`, `parms`, and `cp`. This is to ensure the architecture and size of tree which can give the best performance.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-dt-params
#| tbl-cap: The performance summary of Decision Tree Hyperpamater Tuning.

# import libraries
library(rpart)
library(rpart.plot)
library(MLmetrics)

# create empty dataframe
results <-data.frame()

# iterate maxdepth and minsplit
for (maxdepth in c(5,10,15,20,25,30))
  for (minsplit in c(5,10,15,20,25,30)) {
    set.seed(123)
    DT_classifier <-  rpart(
      y ~ ., 
      data=train, 
      method="class",
      parms = list(prior = c(0.7,0.3)), # higher weight for minority class
      control=rpart.control(cp = -1, 
                            minsplit = minsplit, 
                            maxdepth = maxdepth))
    
    # predict to the train dataset, using F-score
    train.pred <- predict(DT_classifier, 
                          newdata=subset(train, select = -c(y)),
                          type="class")
    train$y <- factor(train$y, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, train$y, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(DT_classifier, 
                          newdata=subset(valid, select = -c(y)),
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    max.depth <- array(maxdepth, dim = c(1, 1))
    min.split <- array(minsplit, dim = c(1, 1))
    fscore_train <- array(f_score.train, dim = c(1, 1))
    fscore_valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(max.depth,min.split,fscore_train,fscore_valid)
    results <- rbind(results, result)
}

# convert as dataframe
results <- as.data.frame(results)
colnames(results) <- c("maxdepth","minsplit","Fscore_train","Fscore_valid")

# print top-3 F-score for validation dataset
results %>% 
  top_n(n = 3, wt = Fscore_valid) %>%
  kable()
```

```{r}
#| echo: false
#| warning: false
#| label: fig-dt-cp
#| fig-cap: Hyperparameter tuning for CP Decision Tree
#| fig-align: center
#| message: false

set.seed(123)

# Build full Decision Trees using minsplit, maxdepth from the previous step
DT_full_classifier <-  rpart(
  y ~ ., 
  data=train, 
  method="class",
  parms = list(prior = c(0.70,0.30)),
  control=rpart.control(cp = -1,
                        minsplit = 5, 
                        maxdepth = 5)
)
plotcp(DT_full_classifier)

# Get the most optimum CP by using minimum error strategy
cptable <- as.data.frame(DT_full_classifier$cptable)
min_index <- which.min(cptable$xerror)
opt.cp <- cptable[min_index, "CP"]
```

In this case, more weight for minority class is defined by `parms = list(prior = c(0.7,0.3))` as we have imbalance classification problem. After some investigations (iterative process), `maxdepth` = 5 and `minsplit` = 5 can return highest validation F1 score. After that, using built-in package from `rpart`, we have to prune the tree using `cp` = 0.0005585 as returning lowest cross-validation error rate (`xerror`).

```{r}
#| echo: false
#| warning: false
#| label: fig-dt-tree
#| fig-cap: Decision Tree with best hyperparameters
#| fig-align: center
#| message: false

# Build model with best params
set.seed(123)
DT_prune_classifier <- prune(DT_full_classifier, cp=opt.cp)
rpart.plot(DT_prune_classifier,type=2,extra=4)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-dt-performance
#| tbl-cap: The performance summary of Decision Tree.

# Create row names
dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(DT_prune_classifier, 
                      newdata=subset(train, select = -c(y)),
                      type="class")
train$y <- factor(train$y, levels = levels(train.pred))

# Measure performance to train dataset
f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to validation dataset
valid.pred <- predict(DT_prune_classifier, 
                      newdata=subset(valid, select = -c(y)),
                      type="class")
valid$y <- factor(valid$y, levels = levels(valid.pred))

# Measure performance to validation dataset
f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

# Create dataframe for the performance summary
performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))
performance_summary %>%
   kable()
```

Overall, F1 score for both train and validation dataset is close to each other indicating the model is not over- or under-fitting. However, the performance is not that good, around \~50%.

## Random Forest

Random Forest is an ensemble method that combine together a bunch of classification trees on each bootstrapped samples. The prediction from each tree is recorded, and then the final prediction will be the majority votes from all trees. In this case, Random Forest uses different set of random features for each tree to build uncorrelated trees. This is to further reduce the variance.

The hyper-parameters explored are `ntree`, `nodesize`, and `classwt`. In order to deal with imbalanced dataset, the higher class weight is given for minority class by defining `class_weights <- c(0.70, 0.30)`. For `mtyr` is decided to be default sqrt(p) as no major impact from several trial and error.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance
#| tbl-cap: The performance summary of Random Forest.

library(randomForest)

# Define class weight
class_weights <- c(0.70, 0.30) # higher for minority class

# Create empty dataframe
results <-data.frame()
for (nodesize in c(5,10,15,20,30,50))
  for (ntree in c(5,10,30,50,100,200,300)) {
    set.seed(123)
    RF_classifier <- randomForest(y ~ .,
                                  data=train,
                                  ntree=ntree,
                                  classwt=class_weights,
                                  nodesize=nodesize)
    
    # predict to the train dataset, using F-score
    train.pred <- predict(RF_classifier, 
                          newdata=subset(train, select = -c(y)),
                          type="class")
    train$y <- factor(train$y, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, train$y, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(RF_classifier, 
                          newdata=subset(valid, select = -c(y)),
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    no.tree <- array(ntree, dim = c(1, 1))
    node.size <- array(nodesize, dim = c(1, 1))
    fscore.train <- array(f_score.train, dim = c(1, 1))
    fscore.valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(node.size, no.tree, fscore.train, fscore.valid)
    results <- rbind(results, result)
}

# convert to dataframe
results <- as.data.frame(results)
colnames(results) <- c("nodesize","ntree","Fscore_train","Fscore_valid")

results %>% 
  top_n(n = 3, wt = Fscore_valid) %>%
  kable()
```

According to the F1 score, `ntree` = 100 and `nodesize` = 50 is the one that can give highest validation score and its values also close with training score, meaning not over-fitting.

```{r}
#| echo: false
#| warning: false

# Build model with best params
set.seed(123)
class_weights <- c(0.70, 0.30)
RF_classifier_best <- randomForest(y ~ .,
                                   data=train,
                                   classwt=class_weights,
                                   ntree=100,
                                   nodesize=50)
```

```{r}
#| echo: false
#| warning: false

# Create row names
dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(RF_classifier_best, 
                      newdata=subset(train, select = -c(y)),
                      type="class")
train$y <- factor(train$y, levels = levels(train.pred))

# Measure performance to train dataset
f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to validation dataset
valid.pred <- predict(RF_classifier_best, 
                      newdata=subset(valid, select = -c(y)),
                      type="class")
valid$y <- factor(valid$y, levels = levels(valid.pred))

# Measure performance to validation dataset
f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

# Create dataframe for the performance summary
performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

The F1 score between training and validation score is very close to each other, but they are quite low, which is around 50%.

## KNN

```{r}
#| echo: false
#| warning: false

library(class)

set.seed(123)

K <- c(1:15)
cv.corr <- c()
for (k in K){
  train.pred <- knn.cv(train, train$y, k=k)
  cv.corr[k] <- mean(train$y == train.pred)
}

plot(K, cv.corr, type="b", ylab="leave-one-out cross-validation correct classification rate")

```

```{r}
#| echo: false
#| warning: false

set.seed(123)

KNN_classifier <- knn.cv(train, train$y, k=6)
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Train","Validation")

set.seed(123)
# Prediction to train dataset
train.pred <- knn(train, train, train$y, k=6)
train$y <- factor(train$y, levels = levels(train.pred))

f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

set.seed(123)
# Prediction to valid dataset
valid.pred <- knn(train, valid, train$y, k=6)
valid$y <- factor(valid$y, levels = levels(valid.pred))

f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

## LQA

```{r}
#| echo: false
#| warning: false

library(MASS)

LDA_classifier <- lda(y~., data=train)

LDA_classifier.pred.tr <- predict(LDA_classifier)

dataset <- data.frame(Type=train$y, lda=LDA_classifier.pred.tr$x)
ggplot(dataset, aes(x=LD1)) + 
  geom_density(aes(group=Type, colour=Type, fill=Type), alpha=0.3)
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Train","Validation")

set.seed(123)
# Prediction to train dataset
train.pred <- predict(LDA_classifier, train)
#train$y <- factor(train$y, levels = levels(train.pred$class))

f_score <- c(F1_Score(train.pred$class, train$y, positive="1"))
precision <- c(Precision(train.pred$class, train$y, positive="1"))
recall <- c(Recall(train.pred$class, train$y, positive="1"))

set.seed(123)
# Prediction to valid dataset
valid.pred <- predict(LDA_classifier, valid)
#valid$y <- factor(valid$y, levels = levels(valid.pred$class))

f_score <- c(f_score, F1_Score(valid.pred$class, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred$class, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred$class, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

## QDA

```{r}
#| echo: false
#| warning: false

library(MASS)

QDA_classifier <- qda(y~., data=train[,c(numeric_cols,"y")])
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Train","Validation")

set.seed(123)
# Prediction to train dataset
train.pred <- predict(QDA_classifier, train[,c(numeric_cols,"y")])
#train$y <- factor(train$y, levels = levels(train.pred$class))

f_score <- c(F1_Score(train.pred$class, train$y, positive="1"))
precision <- c(Precision(train.pred$class, train$y, positive="1"))
recall <- c(Recall(train.pred$class, train$y, positive="1"))

set.seed(123)
# Prediction to valid dataset
valid.pred <- predict(QDA_classifier, valid[,c(numeric_cols,"y")])
#valid$y <- factor(valid$y, levels = levels(valid.pred$class))

f_score <- c(f_score, F1_Score(valid.pred$class, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred$class, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred$class, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

## SVM


SVM is a powerful classification algorithm that works well for both linear and nonlinear data. It tries to find the hyperplane that best separates different classes in the feature space.

```{r}
#| echo: false
#| warning: false

set.seed(123)

cost_range <- c(0.01,0.1,1,10,100)
degree_range <- 2:5
gamma_range <- c(0.01,0.1,1,10,100)
```

```{r}
#| echo: false
#| warning: false

library("e1071")

set.seed(123)

cost_range <- c(0.01,0.1,1,10,100)
degree_range <- 2:5

SVM_poly <- tune.svm(as.factor(y)~., data=train, type="C-classification", kernel="polynomial", cost=cost_range, degree=degree_range)
summary(SVM_poly)
```

In our SVM parameter tuning procedure with SVM_poly, we used 10-fold cross-validation to test various polynomial degree and cost parameter combinations. Notably, the combination of degree 2 and cost 0.1 resulted in the lowest error rate of 0.1100154, indicating best performance. Moving further, we will investigate another SVM variation, SVM_RBF.

```{r}
#| echo: false
#| warning: false

library("e1071")

set.seed(123)

cost_range <- c(1,10,100)
gamma_range <- c(0.01,0.1,1,10,100)
str(train)

SVM_RBF <- tune.svm(as.factor(y)~., data=train, type="C-classification", kernel="radial", cost=cost_range, gamma=gamma_range)
summary(SVM_RBF)
```

Based on the error rates obtained, the SVM model with the polynomial kernel (SVM_poly) performed marginally better (0.1100154) than the SVM model with the RBF kernel (SVM_RBF), with a lower error rate (0.1112725). Now after extracting the optimal degree and cost parameters obtained from the hyperparameter tuning process, we can do the performance on the test set.

```{r}
#| echo: false
#| warning: false

library("e1071")

degree.opt <- SVM_poly$best.parameters[1]
cost.opt <- SVM_poly$best.parameters[2]

SVM_classifier <- svm(as.factor(y)~., data=train, type="C-classification", kernel="polynomial", degree=degree.opt, cost=cost.opt)

test_pred <- predict(SVM_classifier, newdata = test)

confusion_matrix <- table(test$y, test_pred)
print(confusion_matrix)
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(SVM_classifier,train)
train$y <- factor(train$y, levels = levels(train.pred))

f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to valid dataset
valid.pred <- predict(SVM_classifier,valid)
valid$y <- factor(valid$y, levels = levels(valid.pred))

f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

The SVM classifier performs moderately well on both datasets, with F-scores of 0.2706 and 0.2913 for the training and validation sets, respectively. While the recall values are reasonably high (0.7143 for training and 0.7551 for validation), showing good positive instance capture, the precision is low (0.1669 for training and 0.1805 for validation), indicating a larger rate of false positives. Further fine-tuning, particularly focusing on hyperparameters and potential feature engineering, is required to improve Precision while maintaining Recall.

# Model Selection

```{r}
#| echo: false
#| warning: false

dataset <- c("DecisionTree","RandomForest","KNN","LDA","QDA","SVM")

# Prediction to test dataset

# Decision Tree

test.pred <- predict(DT_prune_classifier, 
                      newdata=subset(test, select = -c(y)),
                      type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

# RF

test.pred <- predict(RF_classifier_prune, 
                      newdata=subset(test, select = -c(y)),
                      type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

# KNN

set.seed(123)
test.pred <- knn(train, test, train$y, k=6)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

# LDA

set.seed(123)
# Prediction to valid dataset
test.pred <- predict(LDA_classifier, test)
#valid$y <- factor(valid$y, levels = levels(valid.pred$class))

f_score <- c(f_score, F1_Score(test.pred$class, test$y, positive="1"))
precision <- c(precision, Precision(test.pred$class, test$y, positive="1"))
recall <- c(recall, Recall(test.pred$class, test$y, positive="1"))

# QDA

set.seed(123)
# Prediction to valid dataset
test.pred <- predict(QDA_classifier, test[,c(numeric_cols,"y")])
#valid$y <- factor(valid$y, levels = levels(valid.pred$class))

f_score <- c(f_score, F1_Score(test.pred$class, test$y, positive="1"))
precision <- c(precision, Precision(test.pred$class, test$y, positive="1"))
recall <- c(recall, Recall(test.pred$class, test$y, positive="1"))

# SVM

test.pred <- predict(SVM_classifier,test)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

Which one is the best? Which metrics we should prioritize?

# Conclusion

So far, KNN is the best one.

# Next: Improve Performance

## Feature Selection (Boruta)

Link: <https://www.datacamp.com/tutorial/feature-selection-R-boruta>

The algorithm to perform top-down search of relevant features using random forest model by iteratively eliminating irrelevant features by comparing original and random shadow features. Boruta does not need human arbitrary decision.

```{r}
#| echo: false
#| warning: false

library(Boruta)

set.seed(123)

boruta <- Boruta(y~., data = train, doTrace = 2)

boruta_df <- attStats(boruta)
```

```{r}
#| echo: false
#| warning: false

features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))

set.seed(123)

KNN_classifie_2 <- knn.cv(train[,c(features_list,"y")], 
                         train$y, 
                         k=6)
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Test")

set.seed(123)
# Prediction to test dataset
test.pred <- knn(train[,c(features_list,"y")], 
                 test[,c(features_list,"y")], 
                 train$y, k=6)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

## Resampling (SMOTE)

Link:

-   <https://spotintelligence.com/2023/02/17/smote-oversampling-python-r/#How_to_use_SMOTE_in_R>

-   <https://rpubs.com/hwulanayu/smote-in-r>

Traditional ML might not performing well for imbalanced dataset because it will be bias to the majority class. SMOTE algorithm works by selecting minority class observations randomly and then find k nearest minority class neighbors, and then generate new samples by doing interpolation between them.

```{r}
#| echo: false
#| warning: false

library(smotefamily)

set.seed(123)

smote_train <- SMOTE(X = subset(train, select = -c(y)), 
                     target = train[,"y"],
                     dup_size = 2)
smote_train <- smote_train$data # extract only the balanced dataset
smote_train$class <- as.factor(smote_train$class)

table(smote_train$class)
```

```{r}
#| echo: false
#| warning: false

features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))

set.seed(123)

KNN_classifie_3 <- knn.cv(smote_train[,c(features_list,"class")], 
                          smote_train$class, 
                          k=6)
```

```{r}
#| echo: false
#| warning: false

library(MLmetrics)

dataset <- c("Test")

set.seed(123)
# Prediction to test dataset
test.pred <- knn(smote_train[,c(features_list,"class")], 
                 test[,c(features_list,"y")], 
                 smote_train$class, k=6)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```
