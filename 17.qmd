---
title: "The Prediction of Bank Term Deposit Subscription using Machine Learning"
author: "Team 17: Yola Kamalita, Burte Ganbold, Aulia Khoirunnisa, Yuang Tian"
editor: visual
number-sections: true
editor_options: 
  chunk_output_type: inline
format: 
  pdf:
    geometry: "left=2cm, right=2cm, top=2.5cm, bottom=2.5cm"
execute:
  echo: false
  eval: true
  warning: false
  message: false
---

```{r}
#| echo: false
#| warning: false

# Import Libraries

library(tidyverse)
library(tidyr)
library(gt)
library(skimr)
library(knitr)
library(ggplot2)
library(gridExtra)
library(dplyr)
library(GGally)
library(caret)
library(Boruta)
library(rpart)
library(rpart.plot)
library(MLmetrics)
library(randomForest)
library(class)
library(MASS)
library(e1071)
library(smotefamily)
library(cvms)
library(tibble)
library(Boruta)
```

# Introduction {#sec-intro}

Portuguese bank institution launched marketing campaign to promote term deposit subscription. These campaigns are based on phone calls, and more than one contact to the same client is performed. Furthermore, the research question is to make prediction whether the client will subscribe ("yes"/"no") a term deposit or not. There are 20 attributes available in the dataset related to bank client data, last contact of the current campaign information, social-economic, and others.

```{r}
#| echo: false
#| warning: false

# Read CSV from data dir
data <- read_csv("group_17.csv")
```

# Exploratory Data Analysis

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-stats
#| tbl-cap: Summary statistics of the Bank Marketing Dataset.

# Creating Summary statistics with adjusted skim()
my_skim <- skim_with(
  base = sfl(n = length),
  character = sfl(min=NULL,max=NULL,empty=NULL,whitespace=NULL),
  numeric = sfl(p0=NULL, p100=NULL, hist=NULL))
knit_print(my_skim(data))
```

## Missing Values

```{r}
#| echo: false
#| warning: false
#| label: fig-barplot1
#| fig-cap: The percentage of missing values.
#| fig-align: center
#| fig-width: 3
#| fig-height: 2
#| message: false

# Compute percentage of missing values
missing_percentages <- data %>%
  summarise_all(~sum(is.na(.)))/nrow(data)
missing_percentages_sorted <- missing_percentages %>%
  gather(key = "Column", value = "Missing_Percentages") %>%
  arrange(desc(Missing_Percentages)) %>%
  filter(Missing_Percentages>0)

# Plot missing values
ggplot(missing_percentages_sorted, aes(x = reorder(Column, Missing_Percentages), y = Missing_Percentages)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  labs(x = "Variables", y = "Missing Percentage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6)) +
  coord_flip()
```

Variable `default` has missing values around 20%, and the rest are below 5%.

```{r}
#| echo: false
#| warning: false

# Split the dataset into Training, Validation, and Test

set.seed(123)
# Split training 80% and test 20%
index <- sample(1:nrow(data), nrow(data)*0.8)
training <- data[index, ]
test <- data[-index, ]
# Split train 60% and validation 20%
index <- sample(1:nrow(training), nrow(training)*0.75)
train <- training[index, ]
valid <- training[-index, ]
```

## Dataset Split

In this analysis, the data will be splitted into train (60%), validation (20%), and test (20%). Train and validation are used to train the model and find the best hyperparameters. Finally, model comparison will be done using test dataset, which is unseen from all the models during the fitting process.

## Imbalance Classification Problem

```{r}
#| echo: false
#| warning: false
#| label: fig-barplot2
#| fig-cap: The subcribe count.
#| fig-align: center
#| fig-width: 3
#| fig-height: 1.25
#| message: false

# Check the subscribe or response variable imbalance or not
ggplot(train, aes(x = y)) +
  geom_bar() +  # Create a bar plot
  labs(x = "Subscribe", y = "Count") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6)) +
  coord_flip()
```

There is a huge gap between subscribe yes (1) or not (0). This is going to be the binary response variable.

## Relationship between Variables

From @fig-boxplot1, some variables are able to distinguish subscribe yes or no, such as duration, number of employees, and euribor. Then, some variables from @fig-barplot3 show that there is noticeable different in terms of percentage of subscribe yes between each own categories. It indicates some variables are useful to give signal whether the client will subscribe or not.

However, based on the density plot @fig-scatterplot, the subscribe yes and no overlaps to each other across the continuous variables. The continuous variables' values are also skewed. Moreover, there are some significant correlations, which are considerably moderate to strong. Those variables belong to social-economic attributes.

```{r}
#| echo: false
#| warning: false
#| label: fig-boxplot1
#| fig-cap: The relationship between binary response and continuous explanatory variables.
#| fig-align: center
#| fig-width: 4
#| fig-height: 5
#| message: false

# Creating Boxplot

p1 <- ggplot(training, aes(x=y, y=age)) + geom_boxplot() +
  labs(x="subscribe",y="Age") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p2 <- ggplot(training, aes(x=y, y=duration)) + geom_boxplot() +
  labs(x="subscribe",y="Duration") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p3 <- ggplot(training, aes(x=y, y=campaign)) + geom_boxplot() +
  labs(x="subscribe",y="Campaign") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p4 <- ggplot(training, aes(x=y, y=pdays)) + geom_boxplot() +
  labs(x="subscribe",y="Pdays") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p5 <- ggplot(training, aes(x=y, y=previous)) + geom_boxplot() +
  labs(x="subscribe",y="Previous") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p6 <- ggplot(training, aes(x=y, y=emp.var.rate)) + geom_boxplot() +
  labs(x="subscribe",y="Emp. Variation Rate") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p7 <- ggplot(training, aes(x=y, y=cons.price.idx)) + geom_boxplot() +
  labs(x="subscribe",y="Cons. Price Index") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p8 <- ggplot(training, aes(x=y, y=cons.conf.idx)) + geom_boxplot() +
  labs(x="subscribe",y="Cons. Confidence Index") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p9 <- ggplot(training, aes(x=y, y=euribor3m)) + geom_boxplot() +
  labs(x="subscribe",y="Euribor") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))
p10 <- ggplot(training, aes(x=y, y=nr.employed)) + geom_boxplot() +
  labs(x="subscribe",y="Number of Employees") +
   theme(axis.text.x = element_text(angle = 45, hjust = 1, size=6),
        axis.text.y = element_text(size=6),
        text = element_text(size=6))

# Plot in Grid
grid.arrange(p1, p2, p3, p4, 
             p5, p6, p7, p8,
             p9, p10, ncol=2)
```

```{r}
#| echo: false
#| warning: false
#| label: fig-barplot3
#| fig-cap: The relationship between binary response and categorical explanatory variables.
#| fig-align: center
#| fig-width: 10
#| fig-height: 20
#| message: false

p1 <- ggplot(training, aes(x=job, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Job", y="Percentage", fill="subscribe") +
coord_flip()

p2 <- ggplot(training, aes(x=marital, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Marital", y="Percentage", fill="subscribe") +
coord_flip()

p3 <- ggplot(training, aes(x=education, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Education", y="Percentage", fill="subscribe") +
coord_flip()

p4 <- ggplot(training, aes(x=default, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Default", y="Percentage", fill="subscribe") +
coord_flip()

p5 <- ggplot(training, aes(x=housing, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Housing", y="Percentage", fill="subscribe") +
coord_flip()

p6 <- ggplot(training, aes(x=loan, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Loan", y="Percentage", fill="subscribe") +
coord_flip()

p7 <- ggplot(training, aes(x=contact, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Contact", y="Percentage", fill="subscribe") +
coord_flip()

p8 <- ggplot(training, aes(x=month, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Month", y="Percentage", fill="subscribe") +
coord_flip()

p9 <- ggplot(training, aes(x=day_of_week, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Day of Week", y="Percentage", fill="subscribe") +
coord_flip()

p10 <- ggplot(training, aes(x=poutcome, fill=y)) + geom_bar(position = "fill") +
scale_y_continuous(labels = scales::percent) +
labs(x="Poutcome", y="Percentage", fill="subscribe") +
coord_flip()

# Plot in Grid
grid.arrange(p1, p2, p3, p4, 
             p5, p6, p7, p8,
             p9, p10, ncol=1)
```

```{r}
#| echo: false
#| warning: false
#| label: fig-scatterplot
#| fig-cap: The relationship between continuous explanatory variables.
#| fig-align: center
#| fig-width: 12
#| fig-height: 12
#| message: false

numeric_cols <- names(select_if(training, is.numeric))

ggpairs(training,
        columns=numeric_cols,
        ggplot2::aes(colour = y),
        axisLabels = "none",
        )
```

```{r}
#| echo: false
#| warning: false

# PCA for Socio-economic Features.

# Check standard deviation
#features_sd <- round(sqrt(diag(cov(training[,16:20]))),1)
# Perform PCA
#socio.econ.pca <- princomp(training[,16:20],cor=T)
# Display summary
#summary_of_pca <- summary(socio.econ.pca)
#plot(socio.econ.pca)

# PC1 vs. subscribe
#socio.econ.pca2 <- as.data.frame(socio.econ.pca$scores[,1:2])
#socio.econ.pca2$y <- training$y
#ggplot(socio.econ.pca2, aes(x=c(1:nrow(training)),y=Comp.1,color=as.factor(y))) + 
#  geom_jitter(width = 0.75, height = 0.75) +
#  labs(x='Index', y='Comp.1', title='PCA')
```

# Data Preprocessing {#section-preprocessing}

Here are the steps for data preprocessing:

-   Remove `default` because \~20% of them are NAs, and the rest is only categorized as "no" for having credit in default.

-   Remove `pdays` because most of the values marked as 999.

-   Remove `duration` as this feature is available after the call, so it should not be used for realistic prediction.

-   Remove \~5% of rows containing NAs.

-   Perform PCA to socio-economic features, and replace them with first PC (\~70% of cumulative proportion of variance). PCA is a method to reduce the dimension of correlated variables by creating p linear combinations from p original variables. In this case, correlation-based PCA is used as the variation of standard deviation values.

-   Perform scaling to numerical features.

-   Convert categorical features as dummies (one-hot-encoding).

```{r}
#| echo: false
#| warning: false

# Remove default, pdays, and duration
train <- subset(train, select = -c(default,pdays,duration))
valid <- subset(valid, select = -c(default,pdays,duration))
test <- subset(test, select = -c(default,pdays,duration))

# Remove NAs
train <- na.omit(train)
valid <- na.omit(valid)
test <- na.omit(test)

set.seed(123)
# PCA to socio-economic features
socio.econ.pca <- princomp(train[,13:17],cor=T)
train$pca.socio.economic <- as.data.frame(socio.econ.pca$scores)$'Comp.1'
valid$pca.socio.economic <- as.data.frame(predict(socio.econ.pca,valid[,13:17]))$'Comp.1'
test$pca.socio.economic <- as.data.frame(predict(socio.econ.pca,test[,13:17]))$'Comp.1'

train <- train[, -c(13:17)]
valid <- valid[, -c(13:17)]
test <- test[, -c(13:17)]

# Scaling numerical variables
numeric_cols <- names(select_if(train, is.numeric))

var.mean <- apply(train[, numeric_cols],2,mean) #calculate mean
var.sd <- apply(train[, numeric_cols],2,sd)   #calculate standard deviation

# standardise training, validation and test sets
train.scale <- t(apply(train[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))
valid.scale <- t(apply(valid[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))
test.scale <- t(apply(test[, numeric_cols], 1, function(x) (x-var.mean)/var.sd))

train[, numeric_cols] <- train.scale
valid[, numeric_cols] <- valid.scale
test[, numeric_cols] <- test.scale
  
# Convert categorical to dummies

# Convert response variable to be 0 and 1
train$y <- ifelse(train$y == "yes", 1, 0)
valid$y <- ifelse(valid$y == "yes", 1, 0)
test$y <- ifelse(test$y == "yes", 1, 0)

# Define one-hot encoding function
dummy <- dummyVars(" ~ .", data=train)

# Convert to dummies
train <- data.frame(predict(dummy, newdata=train))
valid <- data.frame(predict(dummy, newdata=valid))
test <- data.frame(predict(dummy, newdata=test))
```

# Model Fitting

## Decision Tree

Decision tree is a non-parametric classification method by creating partitions of the feature space (explanatory variables). After the feature space is divided into disjoint and non-overlapping regions, then the class label is predicted as the class that occurs most frequently in the region.

In this analysis, `rpart` package is being used to build the decision tree. Then, there are four hyper-parameters will be explored, such as: `maxdepth`, `minsplit`, `parms`, and `cp`. This is to ensure the architecture and size of tree which can give the best performance.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-dt-params
#| tbl-cap: Decision Tree Hyperpamater Tuning.

# create empty dataframe
results <-data.frame()

# iterate maxdepth and minsplit
for (maxdepth in c(5,10,15,20,25,30))
  for (minsplit in c(5,10,15,20,25,30)) {
    set.seed(123)
    DT_classifier <-  rpart(
      y ~ ., 
      data=train, 
      method="class",
      parms = list(prior = c(0.7,0.3)), # higher weight for minority class
      control=rpart.control(cp = -1, 
                            minsplit = minsplit, 
                            maxdepth = maxdepth))
    
    # predict to the train dataset, using F-score
    train.pred <- predict(DT_classifier, 
                          newdata=subset(train, select = -c(y)),
                          type="class")
    train$y <- factor(train$y, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, train$y, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(DT_classifier, 
                          newdata=subset(valid, select = -c(y)),
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    max.depth <- array(maxdepth, dim = c(1, 1))
    min.split <- array(minsplit, dim = c(1, 1))
    fscore_train <- array(f_score.train, dim = c(1, 1))
    fscore_valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(max.depth,min.split,fscore_train,fscore_valid)
    results <- rbind(results, result)
}

# convert as dataframe
results <- as.data.frame(results)
colnames(results) <- c("maxdepth","minsplit","Fscore_train","Fscore_valid")

# print top-3 F-score for validation dataset
results %>% 
  top_n(n = 3, wt = Fscore_valid) %>%
  kable()
```

```{r}
#| echo: false
#| warning: false
#| label: fig-dt-cp
#| fig-cap: Hyperparameter tuning for CP Decision Tree
#| fig-align: center
#| fig-width: 4
#| fig-height: 3
#| message: false

set.seed(123)

# Build full Decision Trees using minsplit, maxdepth from the previous step
DT_full_classifier <-  rpart(
  y ~ ., 
  data=train, 
  method="class",
  parms = list(prior = c(0.70,0.30)),
  control=rpart.control(cp = -1,
                        minsplit = 5, 
                        maxdepth = 5)
)
plotcp(DT_full_classifier)

# Get the most optimum CP by using minimum error strategy
cptable <- as.data.frame(DT_full_classifier$cptable)
min_index <- which.min(cptable$xerror)
opt.cp <- cptable[min_index, "CP"]
```

In this case, more weight for minority class is defined by `parms = list(prior = c(0.7,0.3))` as we have imbalance classification problem. After some investigations (iterative process), `maxdepth` = 5 and `minsplit` = 5 can return highest validation F1 score. After that, using built-in package from `rpart`, we have to prune the tree using `cp` = 0.0005585 as returning lowest cross-validation error rate (`xerror`).

```{r}
#| echo: false
#| warning: false
#| label: fig-dt-tree
#| fig-cap: Decision Tree with best hyperparameters
#| fig-align: center
#| fig-width: 5
#| fig-height: 4
#| message: false

# Build model with best params
set.seed(123)
DT_prune_classifier <- prune(DT_full_classifier, cp=opt.cp)
rpart.plot(DT_prune_classifier,type=2,extra=4)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-dt-performance
#| tbl-cap: The performance summary of Decision Tree.

# Create row names
dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(DT_prune_classifier, 
                      newdata=subset(train, select = -c(y)),
                      type="class")
train$y <- factor(train$y, levels = levels(train.pred))

# Measure performance to train dataset
f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to validation dataset
valid.pred <- predict(DT_prune_classifier, 
                      newdata=subset(valid, select = -c(y)),
                      type="class")
valid$y <- factor(valid$y, levels = levels(valid.pred))

# Measure performance to validation dataset
f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

# Create dataframe for the performance summary
performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))
performance_summary %>%
   kable()
```

**Model assessment:**

Overall, F1 score for both train and validation dataset is close to each other indicating the model is not over- or under-fitting.

## Random Forest

Random Forest is an ensemble method that combine together a bunch of classification trees on each bootstrapped samples. The prediction from each tree is recorded, and then the final prediction will be the majority votes from all trees. In this case, Random Forest uses different set of random features for each tree to build uncorrelated trees. This is to further reduce the variance.

The hyper-parameters explored are `ntree`, `nodesize`, and `classwt`. In order to deal with imbalanced dataset, the higher class weight is given for minority class by defining `class_weights <- c(0.70, 0.30)`. For `mtyr` is decided to be default sqrt(p) as no major impact from several trial and error.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance
#| tbl-cap: Random Forest Hyperparameter tuning.

# Define class weight
class_weights <- c(0.70, 0.30) # higher for minority class

# Create empty dataframe
results <-data.frame()
for (nodesize in c(5,10,15,20,30,50))
  for (ntree in c(5,10,30,50,100,200,300)) {
    set.seed(123)
    RF_classifier <- randomForest(y ~ .,
                                  data=train,
                                  ntree=ntree,
                                  classwt=class_weights,
                                  nodesize=nodesize)
    
    # predict to the train dataset, using F-score
    train.pred <- predict(RF_classifier, 
                          newdata=subset(train, select = -c(y)),
                          type="class")
    train$y <- factor(train$y, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, train$y, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(RF_classifier, 
                          newdata=subset(valid, select = -c(y)),
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    no.tree <- array(ntree, dim = c(1, 1))
    node.size <- array(nodesize, dim = c(1, 1))
    fscore.train <- array(f_score.train, dim = c(1, 1))
    fscore.valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(node.size, no.tree, fscore.train, fscore.valid)
    results <- rbind(results, result)
}

# convert to dataframe
results <- as.data.frame(results)
colnames(results) <- c("nodesize","ntree","Fscore_train","Fscore_valid")

results %>% 
  top_n(n = 3, wt = Fscore_valid) %>%
  kable()
```

According to the F1 score, `ntree` = 100 and `nodesize` = 50 is the one that can give highest validation score and its values also close with training score, meaning not over-fitting.

```{r}
#| echo: false
#| warning: false

# Build model with best params
set.seed(123)
class_weights <- c(0.70, 0.30)
RF_classifier_best <- randomForest(y ~ .,
                                   data=train,
                                   classwt=class_weights,
                                   ntree=100,
                                   nodesize=50)
```

```{r}
#| echo: false
#| warning: false

# Create row names
dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(RF_classifier_best, 
                      newdata=subset(train, select = -c(y)),
                      type="class")
train$y <- factor(train$y, levels = levels(train.pred))

# Measure performance to train dataset
f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to validation dataset
valid.pred <- predict(RF_classifier_best, 
                      newdata=subset(valid, select = -c(y)),
                      type="class")
valid$y <- factor(valid$y, levels = levels(valid.pred))

# Measure performance to validation dataset
f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

# Create dataframe for the performance summary
performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

**Model assessment:**

The F1 score between training and validation score is very close to each other, indicating the model is not overfitting. Overall, the precision and recall are quite low around 50% indicating the model struggling to make correct prediction for minority class, either False Positive or False Negative.

## KNN

KNN is a simple and effective classification algorithm. It is non-parametric and there is no assumptions about the underlying data distribution. With a relatively large dataset, KNN can produce reasonable prediction without the need for complex tuning or parameter optimization, other than number of neighbors, k. According to our investigation, starting from `k` = 10, there is not noticeable improvement in terms of the cross validation correct classification rate. It can be seen from @fig-knn-tuning.

```{r}
#| echo: false
#| warning: false
#| label: fig-knn-tuning
#| fig-cap: KNN Hyperparameter Tuning for K, number of neighbors using leave-one-out CV correct prediction score.
#| fig-align: center
#| message: false

set.seed(123)

K <- c(1:30)
cv.corr <- c()
for (k in K){
  train.pred <- knn.cv(subset(train, select=-y), 
                       train$y, 
                       k=k)
  cv.corr[k] <- mean(train$y == train.pred)
}

plot(K, cv.corr, type="b")
```

```{r}
#| echo: false
#| warning: false

# Fit KNN with best params
set.seed(123)
KNN_classifier <- knn.cv(subset(train, select=-y), 
                         train$y, 
                         k=10)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-knn-performance
#| tbl-cap: The performance summary of KNN.

dataset <- c("Train","Validation")

set.seed(123)
# Prediction to train dataset
train.pred <- knn(subset(train, select=-y), 
                  subset(train, select=-y), 
                  train$y, 
                  k=10)
train$y <- factor(train$y, levels = levels(train.pred))

f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

set.seed(123)
# Prediction to valid dataset
valid.pred <- knn(subset(train, select=-y), 
                  subset(valid, select=-y), 
                  train$y, 
                  k=10)
valid$y <- factor(valid$y, levels = levels(valid.pred))

f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()

# Confusion matrix
#conf_matrix <- table(Actual = train$y, Predicted = train.pred)
#cat("Confusion Matrix:\n", conf_matrix, "\n")
#conf_matrix <- table(Actual = valid$y, Predicted = valid.pred)
#cat("Confusion Matrix:\n", conf_matrix, "\n")
```

**Model assessment:**

The performance from KNN is a bit overfitting. It is quite challenging to find k which can balance over- and under- fitting. One thing that we learn F1 score start to become smaller and smaller as the we try to reduce the gap between train and validation. As there are a lot dummy categorical variables, we might need to consider other distance measurement, such as Jaccard Index. The default one in R `class` is Euclidean.

## Linear Discriminant Analysis (LDA)

**Assessment of LDA Assumptions:**

-   Normality

    The presumption of normality is pivotal for the implementation of Linear Discriminant Analysis (LDA). Each numeric predictor within the confines of a class $k$ is expected to align with a Gaussian distribution. From the visual inspection of the density plots, particularly highlighted in @fig-scatterplot), reveals that certain variables exhibit substantial departures from the prototypical bell curve. Specifically, variables such as $\text{pdays}$ and $\text{previous}$ manifest pronounced skewness, with their distributions marked by an acute peak and lack of symmetry. These characteristics intimate a deviation from normality within the classes. Deviations of this nature could potentially compromise the LDA's proficiency in modeling and distinguishing between the different classes.

-   Homoscedasticity

    In the context of LDA, homoscedasticity means that each predictor variable should show similar variability across each category of the response variable. To evaluate this assumption using boxplots, two main components should be emphasized: the range of the boxes, which represents the interquartile range (IQR), and the lengths of the whiskers, which extend to the furthest points that are not considered outliers. Meanwhile, depicted in @fig-boxplot1, it shows the disparities in the spread and size of the boxplots between 'yes' and 'no' responses. This is particularly evident for variables such as $\text{duration}$ and $\text{campaign}$, where the presence of outliers is conspicuous. Such findings may signify a breach of the homoscedasticity assumption for these predictors.

The analytical observations suggest that the assumptions requisite for LDA---normality and homoscedasticity---are not entirely satisfied for certain predictors in the dataset. Meanwhile, in practical terms, while LDA might still be utilized given its capacity to tolerate certain violations in large samples, the analytical prudence we exercise in this scenario will contribute significantly to the integrity of our results. The key lies in the model validation: an empirical assessment that will illuminate the true impact of any assumption deviations on predictive performance. Should the LDA model demonstrate strong predictive capabilities despite theoretical concerns, we may proceed with its application, albeit with an informed awareness of its limitations. Conversely, should the model falter under validation, our insights into the assumptions will guide us toward more suitable analytical techniques.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-lda-performance
#| tbl-cap: The performance summary of LDA.

set.seed(123)

# Run LDA
LDA_classifier <- lda(y~., data=train)

# Prediction on training data
train_pred <- predict(LDA_classifier, train)

# Calculate the performance metrics for the training data
f_score <- c(F1_Score(y_pred=train_pred$class, y_true=train$y, positive="1"))
precision <- c(Precision(y_pred=train_pred$class, y_true=train$y, positive="1"))
recall <- c(Recall(y_pred=train_pred$class, y_true=train$y, positive="1"))

# Predict on validation data
valid_pred <- predict(LDA_classifier, valid)
  
# Calculate the performance metrics for the validation data
f_score <- c(f_score, F1_Score(y_pred=valid_pred$class, y_true=valid$y, positive="1"))
precision <- c(precision, Precision(y_pred=valid_pred$class, y_true=valid$y, positive="1"))
recall <- c(recall, Recall(y_pred=valid_pred$class, y_true=valid$y, positive="1"))

# Display the metrics
performance_summary <- data.frame(Dataset = c("Train", "Validation"), 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))
performance_summary %>%
   kable()
```

**Model assessment:**

The model's performance on the validation set is slightly better than on the training set, which is atypical and could suggest that the model is not overfitting. Meanwhile, the overall low F1 Scores on both could indicate issues with class imbalance, model sensitivity, or lack of relevant features. It can be derived by the non-normal distributions can affect the model's ability to create accurate discriminant functions, which might be contributing to the lower F1 Scores.

## Quadratic Discriminant Analysis (QDA)

In assessing the suitability of Quadratic Discriminant Analysis (QDA) for our dataset, a crucial step is the examination of the covariance structures of each predictor variable within the context of the target classes. QDA, being a more flexible counterpart to Linear Discriminant Analysis (LDA), allows for each class to have its own covariance matrix, capturing the unique spread and correlation of features within each group. This adaptability is particularly advantageous when the assumption of homogeneity of variances (homoscedasticity) across groups -- a prerequisite for LDA -- does not hold. However, the robustness of QDA comes with its own prerequisites; most notably, each class must present a sufficient number of observations to estimate a full-rank, non-singular covariance matrix.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-lda-covariance-matrix
#| tbl-cap: The summary of Covariance Matrix Ranks.

# The number of the numeric predictors
num_of_numeric_cols <- length(numeric_cols)

# Define the covariance matrix
classes <- levels(train$y)
numeric_cols
cov_matrices <- lapply(classes, function(cls) {
  data_cls <- train[train$y == cls, numeric_cols]
  cov_matrix <- cov(data_cls)
  list(cov_matrix = cov_matrix, rank = qr(cov_matrix)$rank)
})

# Check the rank of each covariance matrix
cov_ranks <- sapply(cov_matrices, function(cov_info) cov_info$rank)

summary_df <- data.frame(
  Class = classes,
  Rank = cov_ranks,
  Num_of_Numeric_Predictors = rep(num_of_numeric_cols, length(classes))
)

# Display the summary table
kable(summary_df, caption = "Summary of Numeric Predictors and Covariance Matrix Ranks")
```

**Model assessment:**

-   **Mismatch in Feature Count and Matrix Rank**: The ranks of the covariance matrices (40 and 39) are less than the number of numeric predictors (49). This discrepancy indicates rank deficiency in the dataset for both classes. Rank deficiency suggests that not all 49 features are contributing unique information for modeling each class due to perfect multicollinearity among some features or insufficient data to estimate a unique covariance matrix.

-   **Implication for QDA:** The QDA assumes that each class has its own covariance matrix, which requires sufficient data to estimate. With a rank deficiency indicated by the ranks of 40 and 39 compared to 49 features, there's a clear signal that the data might not support estimating a full and unique covariance matrix for each class. This could compromise the reliability and stability of the QDA model. The evidence suggests that without addressing the rank deficiency issue, *QDA may not be the most appropriate model for this dataset*.

## SVM

SVM is a powerful classification algorithm that works well for both linear and nonlinear data. It tries to find the hyperplane that best separates different classes in the feature space.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-svm-performance-poly
#| tbl-cap: SVM Polynomial Hyperpamater Tuning.

set.seed(123)

cost_range <- c(0.01,0.1,1,10,100)
degree_range <- 2:5

SVM_poly <- tune.svm(as.factor(y)~., data=train, type="C-classification", kernel="polynomial", cost=cost_range, degree=degree_range)

sum_svm_poly <- summary(SVM_poly)

sum_svm_poly$best.parameters %>% 
  kable()
```

In our SVM parameter tuning procedure with SVM Polynomial, we use 10-fold cross-validation to test various polynomial degree and cost parameter combinations. Notably, the combination of degree 2 and cost 0.1 resulted in the lowest error rate of 0.1100154, indicating best performance. Moving further, we will investigate another SVM variation, SVM RBF.

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-svm-performance-rbf
#| tbl-cap: SVM RBF Hyperpamater Tuning.

set.seed(123)

cost_range <- c(0.01,0.1,1,10,100)
gamma_range <- c(0.01,0.1,1,10,100)
# str(train)

SVM_RBF <- tune.svm(as.factor(y)~., data=train, type="C-classification", kernel="radial", cost=cost_range, gamma=gamma_range)

sum_svm_rbf <- summary(SVM_RBF)

sum_svm_rbf$best.parameters %>% 
  kable()
```

Based on the error rates obtained, the SVM model with the polynomial kernel performed marginally better (0.1100154) than the SVM model with the RBF kernel, with a lower error rate (0.1112725). Now after extracting the optimal degree and cost parameters obtained from the hyperparameter tuning process, we can do the performance on the validation set.

```{r}
#| echo: false
#| warning: false

degree.opt <- SVM_poly$best.parameters[1]
cost.opt <- SVM_poly$best.parameters[2]

SVM_classifier <- svm(as.factor(y)~., data=train, type="C-classification", kernel="polynomial", degree=degree.opt, cost=cost.opt)

#test_pred <- predict(SVM_classifier, newdata = test)

#confusion_matrix <- table(test$y, test_pred)
#print(confusion_matrix)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-svm-performance
#| tbl-cap: The performance summary of SVM.

dataset <- c("Train","Validation")

# Prediction to train dataset
train.pred <- predict(SVM_classifier,train)
train$y <- factor(train$y, levels = levels(train.pred))

f_score <- c(F1_Score(train.pred, train$y, positive="1"))
precision <- c(Precision(train.pred, train$y, positive="1"))
recall <- c(Recall(train.pred, train$y, positive="1"))

# Prediction to valid dataset
valid.pred <- predict(SVM_classifier,valid)
valid$y <- factor(valid$y, levels = levels(valid.pred))

f_score <- c(f_score, F1_Score(valid.pred, valid$y, positive="1"))
precision <- c(precision, Precision(valid.pred, valid$y, positive="1"))
recall <- c(recall, Recall(valid.pred, valid$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

**Model assessment:**

The SVM classifier performs moderately well on both datasets, with F-scores of 0.2706 and 0.2913 for the training and validation sets, respectively. While the recall values are reasonably high (0.7143 for training and 0.7551 for validation), showing good positive instance capture, the precision is low (0.1669 for training and 0.1805 for validation), indicating a larger rate of false positives. Further fine-tuning, particularly focusing on hyperparameters and potential feature engineering, is required to improve Precision while maintaining Recall. Another issue could be from a lot of dummy variables.

# Model Selection

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-test-performance
#| tbl-cap: The performance summary of all the models on the test set (unseen before).

dataset <- c("DecisionTree","RandomForest","KNN","LDA","SVM")

# Prediction to test dataset

# Decision Tree

test.pred <- predict(DT_prune_classifier, 
                     newdata=subset(test, select = -c(y)),
                     type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

# RF

test.pred <- predict(RF_classifier_best, 
                     newdata=subset(test, select = -c(y)),
                     type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

# KNN

set.seed(123)
test.pred <- knn(subset(train, select = -c(y)), 
                 subset(test, select = -c(y)), 
                 train$y, 
                 k=10)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

# LDA

set.seed(123)
# Prediction to valid dataset
test.pred <- predict(LDA_classifier, test)
#valid$y <- factor(valid$y, levels = levels(valid.pred$class))

f_score <- c(f_score, F1_Score(test.pred$class, test$y, positive="1"))
precision <- c(precision, Precision(test.pred$class, test$y, positive="1"))
recall <- c(recall, Recall(test.pred$class, test$y, positive="1"))

# SVM

test.pred <- predict(SVM_classifier,test)
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(f_score, F1_Score(test.pred, test$y, positive="1"))
precision <- c(precision, Precision(test.pred, test$y, positive="1"))
recall <- c(recall, Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

Based on @tbl-summary-test-performance, Random Forest reveals highest F1 Score (47.30%) and Precision (49.07%), and then LDA (49.57%) reveals highest Recall. In this scenario, lower Precision indicates higher False Positive. On the other hand, lower Recall indicating higher False Negative. In terms of prediction, higher False Positive will lead the marketing team to call non potential clients and loosing productivity, and higher False Negative will increase the risk of loosing potential clients. The most appropriate model is the one than can balance between those two, in this matter, Random Forest by looking at the F1 score and the small gap between Precision and Recall.

# Conclusion

From the Confusion Matrix @fig-rf-cm, Random Forest shows that only half of True Positive can be correctly predicted. However, the number of FP and FN are quite balance.

## Confusion Matrix

```{r}
#| echo: false
#| warning: false
#| label: fig-rf-cm
#| fig-cap: Random Forest Confusion Matrix.
#| fig-align: center
#| message: false

# Plot confusion matrix
test.pred <- predict(RF_classifier_best, 
                     newdata=subset(test, select = -c(y)),
                     type="class")
test$y <- factor(test$y, levels = levels(test.pred))
cfm <- tibble("actual" = test$y,
              "prediction" = test.pred)
cfm <- table(cfm)
cfm <- as.tibble(cfm)
plot_confusion_matrix(cfm, 
                      target_col = "actual", 
                      prediction_col = "prediction",
                      counts_col = "n")
```

## AUC-ROC

As the accuracy might not be the optimal metrics for imbalance dataset, AUC is the other alternative. Random Forest reveals a quite high value, 0.769 from scale 1.00, on the test dataset. For ROC, the better the model, the closer the line to the upper-left corner will be. In this case, Random Forest ROC line is in the middle between random classifier (straight-line) and perfect classifier.

```{r}
#| echo: false
#| warning: false
#| label: fig-rf-roc
#| fig-cap: Random Forest AUC-ROC.
#| fig-align: center
#| fig-width: 4
#| fig-height: 3
#| message: false

# Plot  AUC-ROC

library(ROCR)

test.pred <- predict(RF_classifier_best, 
                     newdata=subset(test, select = -c(y)),
                     type="prob")
score <- prediction(test.pred[,2],test$y)
perf <- performance(score,"tpr","fpr")
auc <- performance(score,"auc")
perfd <- data.frame(x= perf@x.values[1][[1]], y=perf@y.values[1][[1]]) 

ggplot(perfd, aes(x= x, y=y)) + 
  geom_line() +
  xlab("False positive rate") + ylab("True positive rate") +
  ggtitle(paste("Area under the curve:", round(auc@y.values[[1]], 3)))
```

# Improve Performance

## Feature Selection (Boruta)

The algorithm to perform top-down search of relevant features using random forest model by iteratively eliminating irrelevant features by comparing original and random shadow features. Boruta does not need human arbitrary decision.

```{r}
#| echo: false
#| warning: false

set.seed(123)

boruta <- Boruta(y~., data = train, doTrace = 2)

boruta_df <- attStats(boruta)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance3
#| tbl-cap: The performance summary of Random Forest (Feature Selection).

features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))

# Define class weight
class_weights <- c(0.70, 0.30) # higher for minority class

# Create empty dataframe
results <-data.frame()
for (nodesize in c(5,10,15,20,30,50))
  for (ntree in c(5,10,30,50,100,200,300)) {
    set.seed(123)
    RF_classifier <- randomForest(y ~ .,
                                  data=train[,c(features_list,"y")],
                                  ntree=ntree,
                                  classwt=class_weights,
                                  nodesize=nodesize)
    
    # predict to the train dataset, using F-score
    train.pred <- predict(RF_classifier, 
                          newdata=train[,features_list],
                          type="class")
    train$y <- factor(train$y, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, train$y, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(RF_classifier, 
                          newdata=valid[,features_list],
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    no.tree <- array(ntree, dim = c(1, 1))
    node.size <- array(nodesize, dim = c(1, 1))
    fscore.train <- array(f_score.train, dim = c(1, 1))
    fscore.valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(node.size, no.tree, fscore.train, fscore.valid)
    results <- rbind(results, result)
}

# convert to dataframe
results <- as.data.frame(results)
colnames(results) <- c("nodesize","ntree","Fscore_train","Fscore_valid")

#results %>% 
#  top_n(n = 3, wt = Fscore_valid) %>%
#  kable()
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance2
#| tbl-cap: The performance summary of Random Forest (Feature Selection).

# Build RF using selected features.
features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))
set.seed(123)
class_weights <- c(0.70, 0.30)
RF_classifier_best2 <- randomForest(y ~ .,
                                    data=train[,c(features_list,"y")],
                                    classwt=class_weights,
                                    ntree=30,
                                    nodesize=200)
```

```{r}
#| echo: false
#| warning: false

dataset <- c("Test")

set.seed(123)
# Prediction to test dataset
test.pred <- predict(RF_classifier_best2, 
                     newdata=test[,features_list],
                     type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

**Model assessment:**

One of the way that we can do to improve the performance by only including relevant features. In this case, we re-train Random Forest to the dataset with selected features only, and then F1 Score slightly increases from 47.30% to 48.15%. However, it is not considerably significant.

## Resampling (SMOTE)

Traditional ML might not performing well for imbalanced dataset because it will be bias to the majority class. SMOTE algorithm works by selecting minority class observations randomly and then find k nearest minority class neighbors, and then generate new samples by doing interpolation between them.

```{r}
#| echo: false
#| warning: false

# Generate more samples for minority class using SMOTE

set.seed(123)
smote_train <- SMOTE(X = subset(train, select = -c(y)), 
                     target = train[,"y"],
                     dup_size = 3)
smote_train <- smote_train$data # extract only the balanced dataset
smote_train$class <- as.factor(smote_train$class)
# table(smote_train$class)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance4
#| tbl-cap: The performance summary of Random Forest (Feature Selection).

features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))

# Create empty dataframe
results <-data.frame()
for (nodesize in c(5,10,15,20,30,50))
  for (ntree in c(5,10,30,50,100,200,300)) {
    set.seed(123)
    RF_classifier <- randomForest(class ~ .,
                                  data=smote_train[,c(features_list,"class")],
                                  ntree=ntree,
                                  nodesize=nodesize)
    
    # predict to the train dataset, using F-score
    train.pred <- predict(RF_classifier, 
                          newdata=smote_train[,features_list],
                          type="class")
    smote_train$class <- factor(smote_train$class, levels = levels(train.pred))
    f_score.train <- c(F1_Score(train.pred, smote_train$class, positive="1"))
    
    # predict to the validation dataset, using F-score
    valid.pred <- predict(RF_classifier, 
                          newdata=valid[,features_list],
                          type="class")
    valid$y <- factor(valid$y, levels = levels(valid.pred))
    f_score.valid <- c(F1_Score(valid.pred, valid$y, positive="1"))
    
    # fill in the empty dataframe
    no.tree <- array(ntree, dim = c(1, 1))
    node.size <- array(nodesize, dim = c(1, 1))
    fscore.train <- array(f_score.train, dim = c(1, 1))
    fscore.valid <- array(f_score.valid, dim = c(1, 1))
    result <- cbind(node.size, no.tree, fscore.train, fscore.valid)
    results <- rbind(results, result)
}

# convert to dataframe
results <- as.data.frame(results)
colnames(results) <- c("nodesize","ntree","Fscore_train","Fscore_valid")

# results %>% 
#  top_n(n = 3, wt = Fscore_valid) %>%
#  kable()
```

```{r}
#| echo: false
#| warning: false

# Build RF using SMOTE samples.
features_list <- rownames(filter(boruta_df, decision == 'Confirmed'))
set.seed(123)
RF_classifier_best3 <- randomForest(class ~ .,
                                    data=smote_train[,c(features_list,"class")],
                                    ntree=10,
                                    nodesize=50)
```

```{r}
#| echo: false
#| warning: false
#| label: tbl-summary-rf-performance5
#| tbl-cap: The performance summary of Random Forest (SMOTE Resampling).

dataset <- c("Test")

set.seed(123)
# Prediction to test dataset
test.pred <- predict(RF_classifier_best3, 
                     newdata=test[,c(features_list,"y")],
                     type="class")
test$y <- factor(test$y, levels = levels(test.pred))

f_score <- c(F1_Score(test.pred, test$y, positive="1"))
precision <- c(Precision(test.pred, test$y, positive="1"))
recall <- c(Recall(test.pred, test$y, positive="1"))

performance_summary <- data.frame(Dataset = dataset, 
                                  F_score = round(f_score, digits=4), 
                                  Precision = round(precision, digits=4),
                                  Recall = round(recall, digits=4))

performance_summary %>%
   kable()
```

**Model assessment:**

After retraining Random Forest to the SMOTE dataset, it starts to become overfit and the F1 score performance is also decreasing to 46.04%. Further research on how to resample dataset is required to improve the overall process to handle imbalance dataset.

# References

-   Team, D. (2018) *Boruta feature selection in R*, *DataCamp*. Available at: https://www.datacamp.com/tutorial/feature-selection-R-boruta (Accessed: 22 March 2024). 

-   *RPubs*. Available at: https://rpubs.com/hwulanayu/smote-in-r (Accessed: 22 March 2024). 

-   Olsen, L.R. (2024) *Creating a confusion matrix with cvms*. Available at: https://cran.r-project.org/web/packages/cvms/vignettes/Creating_a_confusion_matrix.html (Accessed: 22 March 2024). 

-   Data Mining and Machine Learning Lecture Notes.

-   Advanced Predictive Modeling Lecture Notes.
